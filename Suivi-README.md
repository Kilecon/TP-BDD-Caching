# TP Docker ‚Äî R√©plication PostgreSQL, Cache Redis & Haute Disponibilit√©

# PARTIE A ‚Äî Mise en place Docker (20 min)

## A2. Lancer les services

```bash
$ docker compose ps   
NAME                          IMAGE                       COMMAND                  SERVICE      CREATED         STATUS         PORTS
tp-bdd-caching-db-primary-1   bitnami/postgresql:latest   "/opt/bitnami/script‚Ä¶"   db-primary   4 seconds ago   Up 3 seconds   0.0.0.0:5432->5432/tcp, [::]:5432->5432/tcp
tp-bdd-caching-db-replica-1   bitnami/postgresql:latest   "/opt/bitnami/script‚Ä¶"   db-replica   3 seconds ago   Up 3 seconds   0.0.0.0:5433->5432/tcp, [::]:5433->5432/tcp
tp-bdd-caching-redis-1        redis:7                     "docker-entrypoint.s‚Ä¶"   redis        4 seconds ago   Up 3 seconds   0.0.0.0:6379->6379/tcp, [::]:6379->6379/tcp
```

# PARTIE B ‚Äî V√©rifier la r√©plication PostgreSQL (30 min)

## B1. V√©rifier le r√¥le des bases

### Primary
```bash
$ docker exec -it tp-bdd-caching-db-primary-1  psql -U app -d appdb
Password for user app: 
psql (18.1)
Type "help" for help.

appdb=> SELECT pg_is_in_recovery();
 pg_is_in_recovery 
-------------------
 f
(1 row)

appdb=>
```
‚û°Ô∏è R√©sultat attendu : `false` OK

### Replica

```bash
$ docker exec -it tp-bdd-caching-db-replica-1 psql -U app -d appdb
Password for user app: 
psql (18.1)
Type "help" for help.

appdb=> SELECT pg_is_in_recovery();
 pg_is_in_recovery 
-------------------
 t
(1 row)
```
‚û°Ô∏è R√©sultat attendu : `true` OK

## B2. Tester la r√©plication

Sur le **primary** :

```bash
$ docker exec -it tp-bdd-caching-db-primary-1  psql -U app -d appdb
Password for user app: 
psql (18.1)
Type "help" for help.

appdb=> CREATE TABLE products(
appdb(>   id SERIAL PRIMARY KEY,
appdb(>   name TEXT NOT NULL,
appdb(>   price_cents INT NOT NULL,
appdb(>   updated_at TIMESTAMP DEFAULT NOW()
appdb(> );
 products(name, price_cents)
VALUES ('Keyboard', 4CREATE TABLE
appdb=>
appdb=> INSERT INTO products(name, price_cents)
appdb-> VALUES ('Keyboard', 4999);
INSERT 0 1
appdb=>
```
Sur la **replica** :

```bash
docker exec -it tp-bdd-caching-db-replica-1 psql -U app -d appdb 
Password for user app: 
psql (18.1)
Type "help" for help.

appdb=> SELECT * FROM products;
 id |   name   | price_cents |         updated_at
----+----------+-------------+----------------------------
  1 | Keyboard |        4999 | 2025-12-16 12:54:13.900338
(1 row)
```

# PARTIE C ‚Äî HAProxy comme point d‚Äôentr√©e DB (20 min)

```bash
$ docker compose restart haproxy
[+] Restarting 1/1
 ‚úî Container tp-bdd-caching-haproxy-1  Started    
```

---  # j'en suis l√†

# PARTIE D ‚Äî API : lectures, √©critures et cache Redis (90 min)
D1. Architecture de l'API
L'API FastAPI impl√©mente le pattern cache-aside avec Redis et utilise la r√©plication PostgreSQL pour distribuer la charge :

Writes (PUT/POST) ‚Üí PostgreSQL primary (port 5432)

Reads (GET) ‚Üí PostgreSQL replica (port 5433) avec cache Redis

Cache ‚Üí Redis (port 6379, TTL 60s)

D2. Impl√©menter le cache Redis
R√®gles
Cl√© : product:{id}

TTL : 60 secondes (compromis optimal pour un catalogue produit)

Peut √™tre augment√© √† 120s si les donn√©es sont tr√®s stables

R√©duit √† 30s pour des donn√©es plus volatiles (prix dynamiques, stock)

Pattern cache-aside :

Tentative de lecture Redis

Si cache miss ‚Üí lecture DB replica

Mise en cache du r√©sultat avec TTL

Test : 1√®re lecture (CACHE MISS)

```powershell
PS> Invoke-RestMethod -Uri "http://localhost:8000/products/1" -Method Get
source  data
------  ----
replica @{id=1; name=Keyboard; price_cents=4999; updated_at=16/12/2025 13:50:58}
```

Logs API :

```powershell
2025-12-16 14:54:10,933 - cache - INFO - [CACHE MISS] product:1
2025-12-16 14:54:10,942 - cache - INFO - [CACHE SET] product:1 with TTL 60s
INFO:     127.0.0.1:49844 - "GET /products/1 HTTP/1.1" 200 OK
```
‚û°Ô∏è R√©sultat : source: "replica" + cache rempli avec TTL 60s ‚úÖ

Test : 2√®me lecture (CACHE HIT)
```powershell
PS> Invoke-RestMethod -Uri "http://localhost:8000/products/1" -Method Get

source data
------ ----
cache  @{id=1; name=Keyboard; price_cents=4999; updated_at=16/12/2025 13:50:58}
```
Logs API :

```powershell
2025-12-16 14:54:12,105 - cache - INFO - [CACHE HIT] product:1
INFO:     127.0.0.1:49845 - "GET /products/1 HTTP/1.1" 200 OK
```
‚û°Ô∏è R√©sultat : source: "cache" + aucune requ√™te SQL ‚úÖ

## D3. Invalidation du cache
Lors d'un PUT /products/:id :

Mise √† jour sur le primary PostgreSQL

Suppression de la cl√© Redis product:{id}

Test : Modification avec invalidation
```powershell
PS> $body = '{"name": "Updated Product", "price_cents": 9999}'
PS> Invoke-RestMethod -Uri "http://localhost:8000/products/1" -Method Put -Body $body -ContentType "application/json"

message                      data                                                                            note
-------                      ----                                                                            ----
Product updated successfully @{id=1; name=Updated Product; price_cents=9999; updated_at=16/12/2025 13:54:13} Next GET may‚Ä¶
```
Logs API :

```powershell
2025-12-16 14:54:13,077 - cache - INFO - [CACHE INVALIDATION] product:1 - deleted: 1
INFO:     127.0.0.1:49846 - "PUT /products/1 HTTP/1.1" 200 OK
```
‚û°Ô∏è R√©sultat : deleted: 1 confirme que la cl√© cache a bien √©t√© supprim√©e ‚úÖ

Test : Lecture imm√©diate apr√®s modification
```powershell
PS> Invoke-RestMethod -Uri "http://localhost:8000/products/1" -Method Get

source  data
------  ----
replica @{id=1; name=Updated Product; price_cents=9999; updated_at=16/12/2025 13:54:13}
```
Logs API :

```powershell
2025-12-16 14:54:15,145 - cache - INFO - [CACHE MISS] product:1
2025-12-16 14:54:15,149 - cache - INFO - [CACHE SET] product:1 with TTL 60s
INFO:     127.0.0.1:49847 - "GET /products/1 HTTP/1.1" 200 OK
```
‚û°Ô∏è R√©sultat : source: "replica" avec la nouvelle valeur 9999, cache invalid√© puis recr√©√© ‚úÖ

## D4. Exp√©rience de coh√©rence
Test : Latence de r√©plication et cache

```powershell
PS> $testBody = '{"name": "Gaming Mouse", "price_cents": 5999}'
PS> Invoke-RestMethod -Uri "http://localhost:8000/test-consistency/1" -Method Post -Body $testBody -ContentType "application/json"

updated_value             : @{name=Gaming Mouse; price_cents=5999}
replica_value_immediately : @{id=1; name=Gaming Mouse; price_cents=5999; updated_at=16/12/2025 13:55:32}
replica_value_after_200ms : @{id=1; name=Gaming Mouse; price_cents=5999; updated_at=16/12/2025 13:55:32}
cached_value              : @{id=1; name=Gaming Mouse; price_cents=5999; updated_at=16/12/2025 13:55:32}
analysis                  : @{immediate_replication_lag=False; explanation=Replication was fast (< 1ms)}
```
Logs API :

```powershell
2025-12-16 14:55:32,775 - cache - INFO - [CACHE INVALIDATION] product:1 - deleted: 0
2025-12-16 14:55:32,981 - cache - INFO - [CACHE MISS] product:1
2025-12-16 14:55:32,982 - cache - INFO - [CACHE SET] product:1 with TTL 60s
INFO:     127.0.0.1:55882 - "POST /test-consistency/1 HTTP/1.1" 200 OK
```
‚û°Ô∏è Observation : Dans cet environnement local, la r√©plication est quasi-instantan√©e (< 1ms) ‚úÖ

‚ùì Question : Pourquoi peut-on lire une ancienne valeur ?
Deux facteurs cr√©ent une fen√™tre d'incoh√©rence potentielle :

1. Latence de r√©plication PostgreSQL (5-50ms typique en production)
PostgreSQL utilise la streaming replication asynchrone :

```text
Primary                          Replica
   ‚îÇ                                ‚îÇ
t=0ms: UPDATE price = 5999          ‚îÇ
   ‚îÇ                                ‚îÇ
t=2ms: Write WAL log                ‚îÇ
   ‚îÇ                                ‚îÇ
t=5ms: Send WAL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ |
   ‚îÇ                                ‚îÇ
   ‚îÇ                          t=10ms: Apply WAL
   ‚îÇ                                ‚îÇ
   ‚ñº                                ‚ñº
   ```
Fen√™tre d'incoh√©rence : Entre t=0ms et t=10ms, une lecture sur la replica retourne encore l'ancienne valeur alors que le primary est d√©j√† √† jour.

2. Cache Redis sans invalidation
Sans l'invalidation du cache (DELETE de la cl√©), Redis servirait la valeur p√©rim√©e pendant toute la dur√©e du TTL (60s).

Avec invalidation correcte (impl√©ment√©e dans l'API) :

Le cache est vid√© imm√©diatement apr√®s le PUT

L'incoh√©rence est limit√©e au seul lag de r√©plication (5-50ms)

Dans nos tests locaux : < 1ms, donc invisible

‚û°Ô∏è Solutions pour la production
Sc√©nario	Solution
Lectures critiques post-√©criture	Lire depuis le primary (session sticky / read-your-writes)
Lectures tol√©rantes	Accepter le lag de 5-50ms sur la replica
√âlimination totale du lag	Utiliser la r√©plication synchrone (‚ö†Ô∏è p√©nalit√© performance)
Notre impl√©mentation avec invalidation cache limite efficacement l'incoh√©rence au seul lag de r√©plication r√©seau, n√©gligeable en environnement local. ‚úÖ
---

# PARTIE E ‚Äî R√©silience : pannes contr√¥l√©es (30 min)

## E1. Panne Redis

```bash
docker compose stop redis
[+] Stopping 1/1
 ‚úî Container tp-bdd-caching-redis-1  Stopped   
```

‚û°Ô∏è L‚ÄôAPI doit continuer √† fonctionner (sans cache).
Cela fonctionne mais attend le timeout de l'appel √† redis. Non tolerable en production.
---

## E2. Panne de la replica

```bash
docker compose stop db-replica
[+] Stopping 1/1
 ‚úî Container tp-bdd-caching-db-replica-1  Stopped 
```

‚û°Ô∏è Choisissez :
- fallback vers primary
- ou erreur explicite
Fallback en d√©grad√© sur primary mais et log 

logs avec les deux docker down :

```powershell
025-12-16 15:15:33,068 - cache - ERROR - [REDIS ERROR] Timeout connecting to server
2025-12-16 15:15:33,068 - database - ERROR - [REPLICA ERROR] could not receive data from server: Software caused connection abort (0x00002745/10053)

2025-12-16 15:15:33,068 - main - ERROR - [REPLICA DOWN] could not receive data from server: Software caused connection abort (0x00002745/10053)
 - Fallback to primary
2025-12-16 15:15:55,195 - cache - ERROR - [CACHE SET FAILED] Timeout connecting to server
INFO:     127.0.0.1:65112 - "GET /products/1 HTTP/1.1" 200 OK
```
---

# PARTIE F ‚Äî Haute Disponibilit√© PostgreSQL (60 min)

## F1. Test : arr√™t du primary

```bash
docker compose stop db-primary
[+] Stopping 1/1
 ‚úî Container tp-bdd-caching-db-primary-1  Stopped    
```

‚û°Ô∏è Les √©critures √©chouent  
‚û°Ô∏è Conclusion : r√©plication ‚â† HA

---

## F2. Promotion de la replica

```bash
docker exec -it db-replica pg_ctl promote -D /bitnami/postgresql/data
```

```sql
SELECT pg_is_in_recovery();
```

‚û°Ô∏è R√©sultat attendu : `false`

```powershell
docker exec -it tp-bdd-caching-db-replica-1 psql -U app -d appdb
Password for user app: 
psql (18.1)
Type "help" for help.

appdb=> SELECT pg_is_in_recovery();
 pg_is_in_recovery 
-------------------
 f
(1 row)
```
---

## F3. Bascule HAProxy

Modifier `haproxy.cfg` :

```cfg
backend pg_primary
  option tcp-check
  tcp-check connect
  server primary db-replica:5432 check
```

```bash
docker compose restart haproxy
[+] Restarting 1/1
 ‚úî Container tp-bdd-caching-haproxy-1  Started  
```
---

## F4. Test de continuit√©

Relancer une √©criture via l‚ÄôAPI.

```powershell
Invoke-RestMethod -Uri "http://localhost:8000/products/1" -Method Put -Body $body -ContentType "application/json"

message                      data                                                                        note
-------                      ----                                                                        ----
Product updated successfully @{id=1; name=New Product; price_cents=1999; updated_at=16/12/2025 14:45:48} Next GET may show stale data due to replication lag
```

‚û°Ô∏è Le service doit refonctionner sans modifier l‚ÄôAPI.

---

## üìù Questions finales (rapport)

1. Diff√©rence entre r√©plication et haute disponibilit√© ?
La r√©plication et la haute disponibilit√© sont deux concepts compl√©mentaires mais distincts que nous avons pu observer concr√®tement dans ce TP.

La r√©plication PostgreSQL que nous avons mise en place consiste √† copier automatiquement les donn√©es du serveur primary vers un ou plusieurs serveurs replica via le m√©canisme de streaming replication. Concr√®tement, chaque modification effectu√©e sur le primary (INSERT, UPDATE, DELETE) g√©n√®re des enregistrements WAL (Write-Ahead Log) qui sont transmis en continu aux replicas. Ces derniers appliquent ces modifications pour maintenir une copie identique des donn√©es. L'objectif principal de la r√©plication est double : d'une part am√©liorer les performances en distribuant la charge de lecture sur plusieurs serveurs, d'autre part assurer une redondance des donn√©es pour limiter le risque de perte. Dans notre TP, nous avons pu constater que les lectures GET passent par la replica sur le port 5433, ce qui d√©charge le primary et permet de scaler horizontalement les capacit√©s de lecture. Nous avons √©galement v√©rifi√© en partie B2 que les donn√©es ins√©r√©es sur le primary apparaissent quasi-instantan√©ment sur la replica, confirmant le bon fonctionnement de la r√©plication.

Cependant, la r√©plication seule ne garantit absolument pas la continuit√© du service. C'est ce que nous avons d√©montr√© en partie F1 lorsque nous avons arr√™t√© le serveur primary avec la commande docker compose stop db-primary. Imm√©diatement, toutes les tentatives d'√©criture PUT ou POST ont √©chou√© avec une erreur de connexion refus√©e, car les replicas PostgreSQL en mode recovery sont en lecture seule et ne peuvent pas accepter d'√©critures. Les lectures ont continu√© de fonctionner normalement via la replica, mais l'application √©tait en mode d√©grad√©, incapable de traiter les modifications de donn√©es. Cette situation peut durer des minutes voire des heures selon le temps n√©cessaire pour qu'un administrateur intervienne.

La haute disponibilit√©, en revanche, est une architecture compl√®te qui vise √† garantir la continuit√© de service m√™me en cas de panne d'un composant critique. Elle repose sur plusieurs piliers : la d√©tection automatique des pannes via des m√©canismes de heartbeat et de health checks, l'√©lection automatique d'un nouveau serveur primary parmi les replicas disponibles, la promotion automatique de ce serveur (√©quivalent de notre pg_ctl promote manuel), et la reconfiguration automatique de l'infrastructure (HAProxy, DNS, etc.) pour router le trafic vers le nouveau primary. Dans notre TP, toutes ces √©tapes ont √©t√© r√©alis√©es manuellement en parties F2 et F3 : nous avons d√ª ex√©cuter manuellement la commande de promotion, √©diter le fichier haproxy.cfg pour changer la cible de db-primary √† db-replica, puis red√©marrer HAProxy. En production avec une solution de haute disponibilit√© comme Patroni, tout ce processus serait automatis√© et prendrait entre 10 et 30 secondes au lieu de plusieurs minutes d'intervention humaine.

La diff√©rence fondamentale est donc que la r√©plication est un m√©canisme de copie de donn√©es passif, tandis que la haute disponibilit√© est une orchestration active qui inclut la r√©plication mais ajoute l'intelligence n√©cessaire pour r√©agir automatiquement aux pannes. Une architecture peut avoir de la r√©plication sans haute disponibilit√© (notre cas dans le TP), mais l'inverse n'est pas possible car la HA n√©cessite des replicas sur lesquels basculer. En termes de garanties, la r√©plication am√©liore la durabilit√© des donn√©es (RPO proche de z√©ro) et les performances, tandis que la haute disponibilit√© am√©liore le temps de r√©cup√©ration (RTO) en √©liminant le besoin d'intervention manuelle.

Un autre aspect important est la pr√©vention du split-brain, un sc√©nario catastrophique o√π deux serveurs se croient simultan√©ment primary suite √† une partition r√©seau. La r√©plication simple n'a aucun m√©canisme pour g√©rer cela. Les solutions de haute disponibilit√© utilisent des syst√®mes de consensus distribu√© comme etcd ou Consul pour s'assurer qu'un seul n≈ìud √† la fois peut √™tre √©lu primary, m√™me en cas de probl√®mes r√©seau complexes.

2. Qu‚Äôest-ce qui est manuel ici ? Automatique ?

Notre architecture pr√©sente un m√©lange int√©ressant d'op√©rations automatiques et manuelles, r√©v√©lant les limites d'une infrastructure sans orchestration compl√®te.

Du c√¥t√© des m√©canismes automatiques qui fonctionnent sans intervention, nous avons d'abord la r√©plication des donn√©es elle-m√™me. Une fois configur√©e via les variables d'environnement POSTGRESQL_REPLICATION_MODE dans le docker-compose, la r√©plication streaming fonctionne de mani√®re totalement transparente. Chaque √©criture sur le primary g√©n√®re des WAL qui sont automatiquement transmis et appliqu√©s sur la replica. Nous l'avons constat√© en partie B2 o√π l'insertion de la ligne Keyboard sur le primary est imm√©diatement visible sur la replica sans aucune action de notre part. Ce m√©canisme continue de fonctionner 24h/24 sans supervision tant que les deux serveurs sont op√©rationnels.

Le cache Redis avec son pattern cache-aside fonctionne √©galement de mani√®re automatique. Lors d'un GET sur un produit, l'API tente d'abord une lecture dans Redis. Si la cl√© existe, elle retourne imm√©diatement la valeur avec une latence de l'ordre de la milliseconde. Si la cl√© n'existe pas (cache miss), l'API interroge automatiquement la replica PostgreSQL, r√©cup√®re les donn√©es, puis les stocke dans Redis avec un TTL de 60 secondes avant de retourner la r√©ponse. Ce cycle lecture-miss-database-cache fonctionne sans intervention et permet d'atteindre des taux de hit sup√©rieurs √† 80 pourcent sur les donn√©es fr√©quemment consult√©es.

L'invalidation du cache apr√®s une modification est √©galement automatis√©e dans notre code Python. Chaque fois qu'un PUT modifie un produit, le code ex√©cute automatiquement un redis.delete sur la cl√© correspondante imm√©diatement apr√®s avoir commit√© la transaction en base. Nous avons pu observer dans les logs l'enchainement syst√©matique UPDATE SQL suivi de CACHE INVALIDATION deleted 1, puis lors du GET suivant un CACHE MISS suivi d'un CACHE SET. Cette orchestration garantit que les donn√©es p√©rim√©es ne restent pas dans le cache apr√®s modification.

Les m√©canismes de r√©silience que nous avons impl√©ment√©s sont √©galement automatiques. Lorsque Redis est arr√™t√© en partie E1, l'API d√©tecte automatiquement l'√©chec de connexion via une exception, log un message REDIS DOWN Fallback to DB, et continue de fonctionner en interrogeant directement la base de donn√©es. De m√™me, lorsque la replica est arr√™t√©e en partie E2, le code catch l'erreur psycopg2.OperationalError, log REPLICA DOWN Fallback to primary, et redirige automatiquement les lectures vers le serveur primary. Ces fallbacks d√©grad√©s permettent de maintenir le service m√™me en cas de panne partielle.

Notre endpoint de health check interroge automatiquement l'√©tat des trois composants Redis, primary et replica √† chaque appel GET /health. Il retourne un statut HTTP 503 Service Unavailable si un composant critique est down, permettant √† des syst√®mes de monitoring externes de d√©tecter les probl√®mes.

En revanche, de nombreuses op√©rations critiques restent totalement manuelles dans notre architecture. La d√©tection des pannes est passive : il n'y a aucun syst√®me qui surveille activement l'√©tat du primary et d√©clenche une alerte si celui-ci devient injoignable. Nous devons nous-m√™mes constater l'√©chec des √©critures pour d√©tecter le probl√®me. Il n'existe pas de m√©canisme de heartbeat qui ping r√©guli√®rement le primary pour v√©rifier sa disponibilit√©.

La promotion d'une replica en primary est enti√®rement manuelle, comme nous l'avons fait en partie F2 avec la commande docker exec -it db-replica pg_ctl promote. Cette op√©ration n√©cessite une connexion SSH au serveur, la connaissance de la commande exacte et du chemin du r√©pertoire de donn√©es PostgreSQL. Dans un contexte de panne √† 3 heures du matin, cette intervention manuelle introduit un d√©lai incompressible de plusieurs minutes.

La reconfiguration de HAProxy est √©galement un processus manuel complexe. Nous devons √©diter le fichier haproxy.cfg, changer la ligne server primary db-primary:5432 en server primary db-replica:5432, sauvegarder le fichier, puis red√©marrer le conteneur HAProxy avec docker compose restart. Pendant cette op√©ration, il y a une br√®ve interruption de service le temps que HAProxy red√©marre et r√©tablisse les connexions. Dans un environnement production, on utiliserait plut√¥t l'API runtime de HAProxy ou un syst√®me de service discovery pour √©viter le red√©marrage.

Apr√®s un failover, la reconstruction d'une nouvelle replica √† partir du nouveau primary est enti√®rement manuelle. Il faudrait soit red√©marrer l'ancien primary en mode replica (en modifiant sa configuration pour pointer vers le nouveau primary), soit provisionner un nouveau serveur et initialiser la r√©plication depuis z√©ro avec pg_basebackup. Cette op√©ration peut prendre des heures pour des bases de plusieurs t√©raoctets.

Le rollback en cas de faux positif (si l'ancien primary revient en ligne alors qu'on a d√©j√† promu la replica) est √©galement manuel et dangereux. Il faut d√©tecter la situation de split-brain potentiel, arr√™ter l'ancien primary, le reconfigurer en replica, et le resynchroniser avec le nouveau primary. Sans proc√©dure stricte, on risque des pertes de donn√©es ou des incoh√©rences.

En production avec Patroni ou une solution √©quivalente, presque toutes ces op√©rations manuelles seraient automatis√©es. Patroni ex√©cute un heartbeat toutes les 10 secondes vers etcd. Si le primary ne r√©pond plus apr√®s trois tentatives (30 secondes), Patroni lance automatiquement une √©lection parmi les replicas disponibles. La replica avec le moins de lag est promue automatiquement en primary. HAProxy, configur√© avec des health checks HTTP sur l'API REST de Patroni, d√©tecte automatiquement le changement et bascule le trafic sans red√©marrage. L'ensemble du failover prend 30 √† 60 secondes sans intervention humaine.

3. Risques cache + r√©plication ?

La combinaison de cache Redis et de r√©plication PostgreSQL introduit plusieurs risques d'incoh√©rence et de performance que nous avons pu observer partiellement dans le TP.

Le premier risque majeur concerne les lectures p√©rim√©es ou stale reads. Ce ph√©nom√®ne d√©coule directement de la nature asynchrone de la r√©plication PostgreSQL. Lorsqu'un client ex√©cute un UPDATE sur le primary, la transaction est valid√©e localement et retourne imm√©diatement un succ√®s. Les enregistrements WAL correspondants sont ensuite transmis via le r√©seau aux replicas et appliqu√©s de mani√®re asynchrone. Ce d√©lai, bien que g√©n√©ralement tr√®s court (5 √† 50 millisecondes en production), cr√©e une fen√™tre temporelle pendant laquelle le primary et les replicas contiennent des donn√©es diff√©rentes. Si pendant cette fen√™tre un autre client effectue une lecture sur la replica, il obtiendra l'ancienne valeur. Dans notre test de coh√©rence en partie D4, nous avons lanc√© l'endpoint test-consistency qui modifie un produit puis lit imm√©diatement depuis la replica. Dans notre environnement local avec des conteneurs sur la m√™me machine, la r√©plication est si rapide (inf√©rieure √† 1 milliseconde) que nous avons obtenu immediate_replication_lag False, mais en production sur des serveurs distants ou avec une charge importante, ce lag peut atteindre plusieurs dizaines de millisecondes voire plusieurs secondes si le r√©seau est satur√© ou si la replica a accumul√© du retard.

Ce probl√®me est amplifi√© par le cache Redis. Imaginons le sc√©nario suivant : un produit est en cache avec un prix de 4999 centimes. Un utilisateur modifie le prix √† 5999 via un PUT. Notre code invalide correctement le cache en ex√©cutant redis.delete. Mais si imm√©diatement apr√®s, un autre utilisateur ex√©cute un GET, notre API tente de lire depuis Redis (cache miss car on vient d'invalider), interroge la replica PostgreSQL, et √† ce moment pr√©cis la replica n'a pas encore appliqu√© le WAL et retourne toujours 4999. Notre code met alors en cache cette valeur p√©rim√©e pendant 60 secondes. R√©sultat : pendant une minute enti√®re, tous les clients liront le mauvais prix m√™me si la r√©plication s'est entre-temps termin√©e. C'est ce qu'on appelle une cache pollution : le cache amplifie temporellement l'incoh√©rence initiale due au lag de r√©plication.

Un autre risque li√© au cache est l'invalidation √©chou√©e. Nous l'avons observ√© en partie E1 lorsque Redis √©tait arr√™t√©. Quand un PUT modifie un produit, le code essaie d'invalider le cache mais Redis ne r√©pond pas (timeout). L'√©criture en base r√©ussit mais l'invalidation √©choue. Si Redis red√©marre quelques secondes plus tard, le cache contient toujours l'ancienne valeur et la servira pendant toute la dur√©e du TTL restant. Dans nos logs, on voit REDIS DOWN Cannot invalidate cache Timeout, mais le PUT retourne quand m√™me un succ√®s 200 OK au client. Ce dernier pense que sa modification est prise en compte, mais les lectures ult√©rieures montrent toujours l'ancienne valeur. Ce type d'incoh√©rence silencieuse est particuli√®rement dangereux car difficile √† d√©tecter et √† debugger.

Le ph√©nom√®ne de cache stampede constitue un risque de performance majeur. Il se produit lorsque le cache expire sur une cl√© tr√®s populaire (par exemple la page d'accueil ou un produit en promotion). √Ä l'instant o√π le TTL de 60 secondes arrive √† expiration, si mille clients simultan√©s tentent de lire ce produit, ils obtiennent tous un cache miss au m√™me moment. Chacun lance alors une requ√™te SQL vers la replica PostgreSQL. R√©sultat : au lieu d'avoir une requ√™te par minute gr√¢ce au cache, on se retrouve avec mille requ√™tes en une fraction de seconde. La replica, m√™me performante, ne peut pas absorber ce pic instantan√©. Les connexions s'accumulent, les temps de r√©ponse explosent, certaines requ√™tes timeout. Pendant ce temps, d'autres produits subissent le m√™me sort √† mesure que leurs caches expirent. C'est un effet domino qui peut mener √† une indisponibilit√© totale du service. Nous ne l'avons pas test√© dans le TP car cela n√©cessiterait un outil de load testing, mais c'est un sc√©nario classique en production.

Les timeouts en cascade repr√©sentent un autre danger. Lorsque Redis est surcharg√© ou red√©marre, chaque tentative de lecture g√©n√®re un timeout de plusieurs secondes. Si notre API a configur√© un socket_timeout de 2 secondes et re√ßoit 100 requ√™tes par seconde, cela signifie que 200 threads ou processus sont bloqu√©s en attente de timeout simultan√©ment. Ces threads consomment de la m√©moire et des ressources syst√®me. Si le nombre de workers de notre serveur uvicorn est limit√© (par exemple 10 workers), ils sont tous rapidement satur√©s en train d'attendre des timeouts Redis, et l'API devient incapable de traiter de nouvelles requ√™tes m√™me si la base de donn√©es fonctionne parfaitement. Nous avons constat√© cet effet en partie E1 o√π les logs montrent REDIS ERROR Timeout connecting to server suivi d'un d√©lai notable avant Fallback to DB. Chaque requ√™te subit ce d√©lai, d√©gradant drastiquement l'exp√©rience utilisateur.

Le risque d'incoh√©rence multi-r√©gion est critique pour les applications globales. Imaginons une architecture avec un primary en Europe et des replicas en Am√©rique et en Asie, chacun avec son propre cache Redis local. Un utilisateur europ√©en modifie un produit. L'invalidation du cache europ√©en est imm√©diate. Mais la r√©plication vers l'Am√©rique prend 50 millisecondes due √† la latence transatlantique, et vers l'Asie 100 millisecondes. Pendant ce temps, les caches am√©ricain et asiatique continuent de servir l'ancienne valeur. Pire encore, si un utilisateur am√©ricain lit le produit pendant cette fen√™tre, son cache local est rempli avec la valeur p√©rim√©e et la servira pendant 60 secondes suppl√©mentaires m√™me apr√®s que la r√©plication soit termin√©e. R√©sultat : des utilisateurs √† travers le monde voient des versions diff√©rentes du m√™me produit pendant une p√©riode prolong√©e.

Un dernier risque souvent sous-estim√© est la corruption silencieuse des donn√©es en cache. Si un bug dans le code de s√©rialisation JSON transforme par erreur un prix de 9999 en 999, et que cette valeur corrompue est mise en cache, elle sera servie √† tous les clients pendant 60 secondes avant que le cache n'expire. Contrairement √† une corruption en base de donn√©es qui affecte une seule ligne modifiable, une corruption dans le cache se propage massivement et dispara√Æt d'elle-m√™me, rendant le debugging quasi impossible. Les logs ne montreront que des lectures normales avec CACHE HIT, sans trace de l'anomalie.

Pour mitiger ces risques, plusieurs strat√©gies existent. Pour les lectures critiques post-√©criture (un utilisateur qui vient de modifier un produit et le relit imm√©diatement), on peut impl√©menter le pattern read-your-writes en lisant temporairement depuis le primary plut√¥t que la replica pendant quelques centaines de millisecondes apr√®s une modification. Pour le cache stampede, on peut utiliser la technique de probabilistic early expiration o√π le cache est recharg√© al√©atoirement juste avant expiration, ou impl√©menter un lock distribu√© avec Redis SETNX pour s'assurer qu'un seul client recharge le cache tandis que les autres attendent. Pour les invalidations √©chou√©es, on peut r√©duire le TTL √† 30 secondes sur les donn√©es critiques, limitant ainsi la dur√©e maximale d'incoh√©rence. On peut aussi impl√©menter un syst√®me de retry avec backoff exponentiel sur l'invalidation, ou passer √† un mod√®le write-through o√π on √©crit simultan√©ment dans le cache et la base. Pour l'incoh√©rence multi-r√©gion, un syst√®me de pub-sub Redis permet de propager les invalidations instantan√©ment √† tous les caches globaux ind√©pendamment de la r√©plication PostgreSQL.

4. Comment am√©liorer cette architecture en production ?
Pour transformer cette architecture de TP en un syst√®me production-ready capable de supporter une charge r√©elle et garantir une disponibilit√© √©lev√©e, plusieurs am√©liorations critiques sont n√©cessaires, organis√©es par ordre de priorit√©.

La premi√®re et plus critique am√©lioration est l'impl√©mentation d'un syst√®me de haute disponibilit√© automatique pour PostgreSQL avec Patroni. Actuellement, notre failover manuel en parties F2-F3 n√©cessite plusieurs √©tapes humaines et prend plusieurs minutes. Patroni est un syst√®me d'orchestration qui transforme ce processus en un m√©canisme enti√®rement automatique. Patroni s'installe sur chaque n≈ìud PostgreSQL et communique via un store distribu√© comme etcd ou Consul. Chaque instance Patroni envoie un heartbeat toutes les 10 secondes vers etcd pour signaler qu'elle est vivante. Si le primary ne parvient pas √† renouveler son heartbeat pendant 30 secondes (param√®tre configurable), les autres n≈ìuds Patroni d√©tectent la panne et lancent automatiquement une √©lection. La replica avec le moins de lag de r√©plication (d√©termin√© en comparant les positions LSN dans les WAL) est automatiquement promue en nouveau primary via l'√©quivalent de notre commande pg_ctl promote. Les autres replicas sont reconfigur√©es automatiquement pour r√©pliquer depuis le nouveau primary. L'ensemble de ce processus prend entre 10 et 30 secondes sans aucune intervention humaine. Patroni expose √©galement une API REST sur chaque n≈ìud qui retourne le statut actuel (primary, replica, ou indisponible), permettant √† HAProxy de router le trafic dynamiquement vers le n≈ìud primary actif via des health checks HTTP plut√¥t que notre configuration statique actuelle. Cette am√©lioration transforme notre RTO (Recovery Time Objective) de plusieurs minutes √† moins d'une minute.

La deuxi√®me am√©lioration essentielle est la haute disponibilit√© pour Redis via Redis Sentinel ou Redis Cluster. Actuellement, si Redis tombe en panne comme en partie E1, notre API passe en mode d√©grad√© avec tous les appels allant directement en base et subissant les timeouts Redis. Redis Sentinel est un syst√®me de surveillance et de failover pour Redis similaire conceptuellement √† Patroni. On d√©ploie au minimum trois instances Sentinel (toujours un nombre impair pour le quorum) qui surveillent un master Redis et une ou plusieurs replicas Redis. Si le master ne r√©pond plus apr√®s un d√©lai configur√© (g√©n√©ralement 5 secondes), les Sentinels votent pour promouvoir automatiquement une replica en nouveau master et reconfigurent les clients. Notre application Python utiliserait alors la biblioth√®que redis-py avec support Sentinel qui d√©couvre automatiquement le master actuel et bascule transparemment en cas de failover. Pour des besoins de scalabilit√© plus avanc√©s, Redis Cluster permet de partitionner les donn√©es sur plusieurs n≈ìuds avec r√©plication et failover automatique, mais pour notre cas d'usage de catalogue produit, Sentinel est largement suffisant.

La troisi√®me am√©lioration critique est l'ajout d'un connection pooler comme PgBouncer entre notre application et PostgreSQL. Actuellement, notre code Python cr√©e des pools de connexions directement vers les bases de donn√©es avec un maximum de 20 connexions par pool. Si nous d√©ployons 10 instances de notre API pour g√©rer la charge, nous avons potentiellement 200 connexions simultan√©es vers le primary (10 instances fois 20 connexions). PostgreSQL g√®re mal un grand nombre de connexions, avec des d√©gradations de performance au-del√† de quelques centaines. PgBouncer est un proxy l√©ger qui se place entre l'application et la base. Il accepte des milliers de connexions c√¥t√© client mais maintient un petit pool de connexions r√©elles vers PostgreSQL (typiquement 25 √† 50). Il utilise le mode transaction pooling o√π une connexion PostgreSQL r√©elle est allou√©e √† un client uniquement pendant la dur√©e d'une transaction, puis imm√©diatement restitu√©e au pool pour servir un autre client. Cela permet de supporter 10000 clients simultan√©s avec seulement 50 connexions PostgreSQL r√©elles. PgBouncer ajoute une latence n√©gligeable (moins d'une milliseconde) et r√©duit drastiquement la charge sur PostgreSQL. Dans notre architecture, on d√©ploierait un conteneur PgBouncer devant HAProxy, et notre application se connecterait √† PgBouncer plut√¥t que directement √† HAProxy.

L'observabilit√© et le monitoring constituent la quatri√®me am√©lioration indispensable. Actuellement, nous n'avons aucune visibilit√© sur les m√©triques critiques de notre syst√®me. Il est essentiel de d√©ployer une stack Prometheus pour collecter les m√©triques et Grafana pour les visualiser. C√¥t√© PostgreSQL, on utiliserait postgres_exporter qui expose des m√©triques comme le nombre de connexions actives, le taux de transactions par seconde, le lag de r√©plication en secondes ou en bytes, le cache hit ratio, le temps moyen des requ√™tes, etc. C√¥t√© Redis, redis_exporter fournit le nombre de cl√©s, le hit rate du cache, la m√©moire utilis√©e, le nombre de commandes par seconde. C√¥t√© application, on instrumenterait notre code Python avec prometheus_client pour exposer des m√©triques custom comme le nombre de cache hits versus misses, la distribution des temps de r√©ponse par endpoint, le taux d'erreurs, le nombre de fallbacks vers le primary quand la replica est down. Toutes ces m√©triques seraient centralis√©es dans Prometheus avec des dashboards Grafana permettant de visualiser l'√©tat du syst√®me en temps r√©el. Plus important encore, on configurerait des alertes dans Prometheus Alertmanager : alerte si le lag de r√©plication d√©passe 100 millisecondes, alerte si le hit rate du cache tombe sous 70 pourcent, alerte si le taux d'erreur 5xx d√©passe 1 pourcent, alerte si aucun primary n'est disponible. Ces alertes seraient envoy√©es vers PagerDuty ou Opsgenie pour notifier l'√©quipe on-call 24/7.

La cinqui√®me am√©lioration concerne la r√©silience r√©seau et la distribution g√©ographique avec un d√©ploiement multi-AZ (Availability Zone). Actuellement, tous nos conteneurs tournent sur une seule machine. En production cloud (AWS, GCP, Azure), on d√©ploierait le primary dans une zone de disponibilit√© (par exemple eu-west-1a), une premi√®re replica dans une autre zone (eu-west-1b), et une seconde replica dans une troisi√®me zone (eu-west-1c). Chaque AZ est un datacenter physiquement s√©par√© avec son propre r√©seau et alimentation √©lectrique, mais dans la m√™me r√©gion g√©ographique avec une latence inf√©rieure √† 2 millisecondes. Cette configuration permet de survivre √† la panne compl√®te d'un datacenter (incendie, coupure √©lectrique, etc.) tout en maintenant une latence acceptable pour la r√©plication synchrone. On configurerait PostgreSQL avec synchronous_commit remote_apply et synchronous_standby_names ANY 1 pour garantir qu'au moins une replica dans une AZ diff√©rente a appliqu√© chaque transaction avant de retourner un succ√®s au client. Cela √©limine tout risque de perte de donn√©es en cas de panne du primary tout en maintenant des performances acceptables.

La sixi√®me am√©lioration est l'impl√©mentation de m√©canismes anti-cache-stampede dans notre code applicatif. Actuellement, quand un cache expire sur une cl√© populaire, tous les clients lancent simultan√©ment une requ√™te DB. On pourrait impl√©menter un lock distribu√© avec Redis SETNX : le premier client √† d√©tecter le cache miss acquiert un lock, recharge les donn√©es depuis la DB, met √† jour le cache, puis lib√®re le lock. Les autres clients d√©tectent que le lock est pris, attendent quelques millisecondes, puis relisent le cache qui entre-temps a √©t√© recharg√© par le premier client. Une autre approche est le probabilistic early expiration : au lieu d'attendre que le TTL expire compl√®tement √† 60 secondes, on recalcule le cache de mani√®re probabiliste entre 50 et 60 secondes. Par exemple, si le TTL restant est de 5 secondes sur un TTL total de 60, on a une probabilit√© de 5/60 soit environ 8 pourcent de recharger le cache de mani√®re anticip√©e. Cela √©tale dans le temps les rechargements de cache au lieu de les concentrer tous au moment de l'expiration, √©liminant le pic de charge.

La septi√®me am√©lioration concerne la s√©curit√© et l'encryption. Actuellement, nos mots de passe PostgreSQL et nos donn√©es transitent en clair dans docker-compose.yml et sur le r√©seau. En production, on utiliserait des secrets managers comme HashiCorp Vault, AWS Secrets Manager, ou Kubernetes Secrets pour stocker les credentials. Les connexions PostgreSQL seraient s√©curis√©es avec TLS/SSL en configurant ssl_mode require dans les param√®tres de connexion. Le trafic de r√©plication entre primary et replicas serait √©galement chiffr√©. On activerait l'encryption at rest dans PostgreSQL pour chiffrer les donn√©es sur disque. Redis supporterait √©galement TLS pour les connexions clients. On impl√©menterait un WAF (Web Application Firewall) devant l'API pour prot√©ger contre les injections SQL, XSS, et autres attaques courantes. On ajouterait de l'authentification sur notre API (JWT tokens, OAuth2) plut√¥t que de laisser tous les endpoints publics comme actuellement.

La huiti√®me am√©lioration est l'optimisation des performances avec des indexes appropri√©s et du query tuning. Actuellement, notre table products a un index sur la cl√© primaire id, ce qui rend les requ√™tes SELECT WHERE id tr√®s rapides. Mais si notre application devait filtrer par name ou price_cents, ces requ√™tes feraient des full table scans. On cr√©erait des index sur ces colonnes. On utiliserait EXPLAIN ANALYZE sur les requ√™tes fr√©quentes pour identifier les slow queries et optimiser les plans d'ex√©cution. On configurerait pg_stat_statements pour tracker automatiquement les requ√™tes les plus co√ªteuses. C√¥t√© Redis, on passerait d'un TTL fixe de 60 secondes √† des TTL adaptatifs : 120 secondes pour les produits peu modifi√©s, 30 secondes pour les produits fr√©quemment mis √† jour. On pourrait √©galement impl√©menter un cache L1 in-memory au niveau de l'application (un simple dictionnaire Python) pour les donn√©es ultra-fr√©quentes, √©vitant m√™me l'appel r√©seau √† Redis qui prend 1-2 millisecondes.

La neuvi√®me am√©lioration serait l'architecture √©v√©nementielle avec Kafka ou RabbitMQ pour d√©coupler les composants. Plut√¥t que d'invalider le cache de mani√®re synchrone dans le endpoint PUT (ce qui bloque le client si Redis est lent), on publierait un √©v√©nement ProductUpdated dans Kafka. Un worker asynchrone consommerait cet √©v√©nement et invaliderait le cache. Cela rend l'API plus r√©active et permet de scaler ind√©pendamment la partie √©criture (API) et la partie invalidation (workers). On pourrait avoir plusieurs workers d'invalidation pour absorber les pics de modifications. Kafka garantit √©galement la livraison des messages m√™me en cas de panne temporaire des workers, √©vitant les invalidations perdues que nous avons observ√©es en partie E1. On pourrait impl√©menter le pattern Event Sourcing o√π chaque modification g√©n√®re un √©v√©nement immutable stock√© dans Kafka, permettant de reconstruire l'√©tat de n'importe quel produit √† n'importe quel moment dans le temps.

La dixi√®me et derni√®re am√©lioration majeure serait l'impl√©mentation de backups automatis√©s et de disaster recovery. Actuellement, si notre primary subit une corruption de donn√©es (bug applicatif qui DELETE toutes les lignes, ransomware, etc.), la r√©plication propage instantan√©ment cette corruption aux replicas. Toutes nos donn√©es sont perdues. On mettrait en place des backups incr√©mentaux quotidiens avec pg_basebackup et archivage des WAL dans un stockage objet comme S3. On configurerait PostgreSQL avec archive_mode on et archive_command pour envoyer automatiquement chaque segment WAL vers S3 d√®s qu'il est rempli. Cela permet un RPO de quelques secondes : on peut restaurer la base √† n'importe quel point dans le temps. On testerait r√©guli√®rement ces backups en restaurant sur un environnement de staging pour s'assurer qu'ils sont valides. On impl√©menterait √©galement une r√©plication g√©ographique asynchrone vers une autre r√©gion cloud (par exemple de eu-west vers us-east) pour survivre √† la destruction compl√®te d'une r√©gion. Redis serait √©galement backup√© avec RDB snapshots ou AOF (Append Only File).

Ces dix am√©liorations transformeraient notre architecture de TP fonctionnelle en un syst√®me production-ready capable de garantir 99.9 pourcent d'uptime (moins de 9 heures d'indisponibilit√© par an) voire 99.99 pourcent (moins d'une heure par an) avec les composants les plus avanc√©s comme le multi-r√©gion. Le co√ªt et la complexit√© augmentent √©videmment proportionnellement, mais pour un service critique g√©n√©rant des revenus significatifs, ces investissements sont rapidement rentabilis√©s par la r√©duction des pertes li√©es aux pannes et la meilleure exp√©rience utilisateur permise par les performances optimis√©es.
---

## üìä Bar√®me indicatif /20

- Docker & lancement : 3
- R√©plication : 5
- Cache Redis : 5
- R√©silience : 3
- Haute disponibilit√© : 4

---

## üöÄ Bonus
- Anti cache-stampede
- Failover automatique (Patroni)
- HA Redis (Sentinel)
